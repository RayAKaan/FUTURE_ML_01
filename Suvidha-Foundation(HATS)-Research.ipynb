{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RayAKaan/FUTURE_ML_01/blob/main/Suvidha-Foundation(HATS)-Research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: ROBUST ENVIRONMENT SETUP & INSTALLATION (FIXED VERSION)\n",
        "#\n",
        "# This version addresses common installation failures for 'bert-score' and 'hdbscan'.\n",
        "# RUN THIS CELL FIRST.\n",
        "# ==============================================================================\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "print(\"🔧 Starting robust environment setup...\")\n",
        "\n",
        "# 1. Upgrade pip and clear cache\n",
        "print(\"\\n1. Upgrading pip and clearing cache...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"])\n",
        "print(\"✅ pip upgraded and cache cleared.\")\n",
        "\n",
        "# 2. Uninstall any conflicting versions for a clean slate\n",
        "print(\"\\n2. Uninstalling existing 'transformers', 'accelerate', and 'torch'...\")\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"transformers\", \"accelerate\", \"torch\"])\n",
        "print(\"✅ Uninstallation complete.\")\n",
        "\n",
        "# 3. Install core libraries first. PyTorch is a fundamental dependency.\n",
        "print(\"\\n3. Installing core libraries (torch, transformers, accelerate)...\")\n",
        "try:\n",
        "    print(\"   - Installing torch...\")\n",
        "    # Let pip decide the best torch version for the environment (CPU/GPU)\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"torch\"])\n",
        "    print(\"   ✅ torch installed.\")\n",
        "    print(\"   - Installing accelerate==0.34.2...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"accelerate==0.34.2\"])\n",
        "    print(\"   ✅ accelerate installed.\")\n",
        "    print(\"   - Installing transformers==4.44.2...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers==4.44.2\"])\n",
        "    print(\"   ✅ transformers installed.\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ CRITICAL ERROR installing core libraries: {e}\")\n",
        "    print(\"   Please check the output above. This error must be resolved to continue.\")\n",
        "    # Stop execution if core libraries fail\n",
        "    raise\n",
        "\n",
        "# 4. Install essential build dependencies\n",
        "print(\"\\n4. Installing essential build dependencies...\")\n",
        "# hdbscan requires Cython to be compiled from source.\n",
        "try:\n",
        "    print(\"🔄 Installing cython...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"cython\"])\n",
        "    print(\"✅ Successfully installed cython\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ FAILED to install cython: {e}\")\n",
        "    # This is a critical dependency for hdbscan\n",
        "    raise\n",
        "\n",
        "\n",
        "# 5. Install remaining dependencies one-by-one for maximum clarity\n",
        "print(\"\\n5. Installing remaining dependencies one by one...\")\n",
        "# Updated list of packages with fixes for known issues\n",
        "packages = [\n",
        "    \"datasets==2.21.0\",\n",
        "    \"sentence-transformers==3.0.1\",\n",
        "    \"evaluate==0.4.3\",\n",
        "    \"rouge-score==0.1.2\",\n",
        "    # FIX 1: Let pip resolve the version for bert-score to avoid dependency conflicts.\n",
        "    \"bert-score\",\n",
        "    \"nltk==3.9.1\",\n",
        "    \"scikit-learn==1.5.2\",\n",
        "    \"umap-learn==0.5.6\",\n",
        "    # FIX 2: hdbscan is installed after cython, its build dependency.\n",
        "    \"pandas==2.2.2\",\n",
        "    \"ipywidgets==8.1.5\",\n",
        "    \"matplotlib==3.9.2\",\n",
        "    \"seaborn==0.13.2\",\n",
        "    \"tqdm==4.66.5\"\n",
        "]\n",
        "\n",
        "failed_packages = []\n",
        "for package in packages:\n",
        "    try:\n",
        "        print(f\"🔄 Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "        print(f\"✅ Successfully installed {package}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ FAILED to install {package}.\")\n",
        "        print(f\"   Error: {e}\")\n",
        "        failed_packages.append(package)\n",
        "\n",
        "# --- Final Report ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🎉 INSTALLATION PROCESS FINISHED\")\n",
        "print(\"=\"*50)\n",
        "if not failed_packages:\n",
        "    print(\"✅ All packages installed successfully!\")\n",
        "    print(\"\\n👉 IMPORTANT: You must now RESTART the runtime.\")\n",
        "    print(\"   Go to the menu: Runtime -> Restart session.\")\n",
        "else:\n",
        "    print(\"⚠️ The following packages failed to install:\")\n",
        "    for pkg in failed_packages:\n",
        "        print(f\"   - {pkg}\")\n",
        "    print(\"\\nPlease try to fix the errors for the failed packages before proceeding.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbxkpwHiYssW",
        "outputId": "4b7f86e5-4656-4ba6-ea1e-aca6da5f9a20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Starting robust environment setup...\n",
            "\n",
            "1. Upgrading pip and clearing cache...\n",
            "✅ pip upgraded and cache cleared.\n",
            "\n",
            "2. Uninstalling existing 'transformers', 'accelerate', and 'torch'...\n",
            "✅ Uninstallation complete.\n",
            "\n",
            "3. Installing core libraries (torch, transformers, accelerate)...\n",
            "   - Installing torch...\n",
            "   ✅ torch installed.\n",
            "   - Installing accelerate==0.34.2...\n",
            "   ✅ accelerate installed.\n",
            "   - Installing transformers==4.44.2...\n",
            "   ✅ transformers installed.\n",
            "\n",
            "4. Installing essential build dependencies...\n",
            "🔄 Installing cython...\n",
            "✅ Successfully installed cython\n",
            "\n",
            "5. Installing remaining dependencies one by one...\n",
            "🔄 Installing datasets==2.21.0...\n",
            "✅ Successfully installed datasets==2.21.0\n",
            "🔄 Installing sentence-transformers==3.0.1...\n",
            "✅ Successfully installed sentence-transformers==3.0.1\n",
            "🔄 Installing evaluate==0.4.3...\n",
            "✅ Successfully installed evaluate==0.4.3\n",
            "🔄 Installing rouge-score==0.1.2...\n",
            "✅ Successfully installed rouge-score==0.1.2\n",
            "🔄 Installing bert-score...\n",
            "✅ Successfully installed bert-score\n",
            "🔄 Installing nltk==3.9.1...\n",
            "✅ Successfully installed nltk==3.9.1\n",
            "🔄 Installing scikit-learn==1.5.2...\n",
            "✅ Successfully installed scikit-learn==1.5.2\n",
            "🔄 Installing umap-learn==0.5.6...\n",
            "✅ Successfully installed umap-learn==0.5.6\n",
            "🔄 Installing pandas==2.2.2...\n",
            "✅ Successfully installed pandas==2.2.2\n",
            "🔄 Installing ipywidgets==8.1.5...\n",
            "✅ Successfully installed ipywidgets==8.1.5\n",
            "🔄 Installing matplotlib==3.9.2...\n",
            "✅ Successfully installed matplotlib==3.9.2\n",
            "🔄 Installing seaborn==0.13.2...\n",
            "✅ Successfully installed seaborn==0.13.2\n",
            "🔄 Installing tqdm==4.66.5...\n",
            "✅ Successfully installed tqdm==4.66.5\n",
            "\n",
            "==================================================\n",
            "🎉 INSTALLATION PROCESS FINISHED\n",
            "==================================================\n",
            "✅ All packages installed successfully!\n",
            "\n",
            "👉 IMPORTANT: You must now RESTART the runtime.\n",
            "   Go to the menu: Runtime -> Restart session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: FINAL ATTEMPT - LETTING CHOOSE THE VERSION\n",
        "#\n",
        "# This cell uninstalls the problematic pinned version and lets pip find\n",
        "# the latest version of hdbscan that can be successfully compiled.\n",
        "# ==============================================================================\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "print(\"🔧 Starting final attempt to install hdbscan...\")\n",
        "\n",
        "# Step 1: Uninstall any failed installation attempts to ensure a clean slate\n",
        "print(\"\\n1. Uninstalling any existing 'hdbscan' versions...\")\n",
        "try:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"hdbscan\"], check=False)\n",
        "    print(\"✅ Cleanup complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"   (Note: Could not uninstall, might not be installed: {e})\")\n",
        "\n",
        "# Step 2: Install hdbscan without a version pin\n",
        "print(\"\\n2. Installing hdbscan by letting pip resolve the version...\")\n",
        "try:\n",
        "    print(\"🔄 Running: pip install hdbscan\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"hdbscan\"])\n",
        "    print(\"\\n✅ SUCCESS! hdbscan was installed successfully.\")\n",
        "    print(\"\\n🎉 ALL PACKAGES ARE NOW INSTALLED!\")\n",
        "    print(\"\\n👉 IMPORTANT: You must now RESTART the runtime for the changes to take effect.\")\n",
        "    print(\"   Go to the menu: Runtime -> Restart session.\")\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"\\n❌ FAILED to install hdbscan automatically.\")\n",
        "    print(\"   This suggests a fundamental incompatibility with the environment.\")\n",
        "    print(\"\\n--- ALTERNATIVE PLAN ---\")\n",
        "    print(\"If hdbscan is not strictly required, you can use 'DBSCAN' from scikit-learn.\")\n",
        "    print(\"It's already installed and provides similar functionality, though not identical.\")\n",
        "    print(\"   from sklearn.cluster import DBSCAN\")\n",
        "    print(\"\\nIf you absolutely need hdbscan, we would need to debug the full compiler output.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-U8FxwSkvA0",
        "outputId": "5bbd320f-d73d-4835-e8dd-5d0de718e2c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Starting final attempt to install hdbscan...\n",
            "\n",
            "1. Uninstalling any existing 'hdbscan' versions...\n",
            "✅ Cleanup complete.\n",
            "\n",
            "2. Installing hdbscan by letting pip resolve the version...\n",
            "🔄 Running: pip install hdbscan\n",
            "\n",
            "✅ SUCCESS! hdbscan was installed successfully.\n",
            "\n",
            "🎉 ALL PACKAGES ARE NOW INSTALLED!\n",
            "\n",
            "👉 IMPORTANT: You must now RESTART the runtime for the changes to take effect.\n",
            "   Go to the menu: Runtime -> Restart session.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: IMPORTS, CONFIGURATION, AND MODEL LOADING (UPDATED)\n",
        "# ==============================================================================\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict\n",
        "from dataclasses import dataclass\n",
        "from collections import defaultdict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from transformers import (AutoTokenizer, AutoModelForSeq2SeqLM,\n",
        "                          GPT2LMHeadModel, GPT2Tokenizer,\n",
        "                          BertTokenizer, BertModel,\n",
        "                          Trainer, TrainingArguments)\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "os.environ['WANDB_DISABLED'] = 'true'\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# --- Device Setup ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "# --- NLTK Data Download ---\n",
        "print(\"📦 Downloading NLTK data...\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "### <-- MODIFICATION START\n",
        "# Add this line for newer versions of NLTK\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "### <-- MODIFICATION END\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# --- Configuration Class ---\n",
        "@dataclass\n",
        "class Config:\n",
        "    # General\n",
        "    output_dir: str = \"./results\"\n",
        "    seed: int = 42\n",
        "\n",
        "    # Dataset\n",
        "    dataset_name: str = \"cnn_dailymail\"\n",
        "    dataset_version: str = \"3.0.0\"\n",
        "    split: str = \"test\"\n",
        "    n_samples: int = 500 # Using 10 for very fast testing\n",
        "\n",
        "    # Preprocessing\n",
        "    min_sent_length: int = 5\n",
        "    max_sent_length: int = 80\n",
        "\n",
        "    # Encoding & Clustering\n",
        "    encoder_model: str = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "    batch_size: int = 16\n",
        "    use_hdbscan: bool = True # Will fallback to KMeans if hdbscan is not available\n",
        "    min_cluster_size: int = 3\n",
        "    max_clusters: int = 6\n",
        "\n",
        "    # MMR Selection\n",
        "    lambda_diversity: float = 0.2\n",
        "    lambda_coverage: float = 0.6\n",
        "    new_topic_bonus: float = 0.35\n",
        "    sentences_to_select: int = 25\n",
        "\n",
        "    # Generation\n",
        "    gen_model: str = \"facebook/bart-large-cnn\"\n",
        "    max_input_length: int = 1024\n",
        "    max_output_length: int = 350\n",
        "    min_output_length: int = 80\n",
        "    num_beams: int = 8\n",
        "    no_repeat_ngram: int = 3\n",
        "    repetition_penalty: float = 1.5\n",
        "\n",
        "    # Evaluation\n",
        "    compute_bertscore: bool = True\n",
        "    compute_perplexity: bool = True\n",
        "\n",
        "    # --- FINE-TUNING SETTINGS ---\n",
        "    # Set this to True to enable fine-tuning\n",
        "    finetune_enabled: bool = True\n",
        "\n",
        "    # Use a smaller model for much faster fine-tuning\n",
        "    finetune_model_checkpoint: str = \"sshleifer/distilbart-cnn-12-6\"\n",
        "\n",
        "    # Fine-tuning hyperparameters\n",
        "    finetune_samples: int = 500\n",
        "    finetune_epochs: int = 2\n",
        "    finetune_batch_size: int = 1 # Use a small batch size to avoid OOM errors\n",
        "    finetune_lr: float = 3e-5\n",
        "    # --------------------------------\n",
        "\n",
        "    # Add a device attribute to the config, using the global 'device' variable\n",
        "    device: str = device\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"This runs after the dataclass is initialized.\"\"\"\n",
        "        print(f\"✅ Config object initialized. Using device: {self.device}\")\n",
        "        if self.finetune_enabled:\n",
        "            print(f\"🔥 Fine-tuning ENABLED. Will use model: {self.finetune_model_checkpoint}\")\n",
        "        else:\n",
        "            print(f\"ℹ️ Fine-tuning DISABLED. Will use model: {self.gen_model}\")\n",
        "\n",
        "# --- Preload Models for Efficiency ---\n",
        "print(\"\\n🚀 Preloading models to save time during evaluation...\")\n",
        "config = Config()\n",
        "encoder_model = SentenceTransformer(config.encoder_model, device=device)\n",
        "generator_tokenizer = AutoTokenizer.from_pretrained(config.gen_model)\n",
        "generator_model = AutoModelForSeq2SeqLM.from_pretrained(config.gen_model).to(device)\n",
        "generator_model.eval()\n",
        "print(\"✅ Encoder and Generator models loaded and ready.\")\n",
        "\n",
        "# --- GPU Optimization ---\n",
        "def optimize_gpu():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"✅ GPU optimization enabled.\")\n",
        "\n",
        "def log_gpu(stage: str):\n",
        "    if torch.cuda.is_available():\n",
        "        alloc = torch.cuda.memory_allocated() / 1024**3\n",
        "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "        print(f\"[GPU] {stage}: Allocated {alloc:.2f} GB, Reserved {reserved:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCYTUB7yVwqC",
        "outputId": "b1c58862-302a-4b5a-969a-bad716a42ec9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using device: cuda\n",
            "📦 Downloading NLTK data...\n",
            "\n",
            "🚀 Preloading models to save time during evaluation...\n",
            "✅ Config object initialized. Using device: cuda\n",
            "🔥 Fine-tuning ENABLED. Will use model: sshleifer/distilbart-cnn-12-6\n",
            "✅ Encoder and Generator models loaded and ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: CORE LOGIC CLASSES\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Preprocessor ---\n",
        "class Preprocessor:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.stopwords = set(stopwords.words('english'))\n",
        "\n",
        "    def preprocess(self, text: str) -> List[Tuple[str, float]]:\n",
        "        sentences = sent_tokenize(text)\n",
        "        sentences = [s for s in sentences if self.config.min_sent_length <= len(s.split()) <= self.config.max_sent_length]\n",
        "        results = []\n",
        "        for i, sent in enumerate(sentences):\n",
        "            words = word_tokenize(sent.lower())\n",
        "            content_words = [w for w in words if w.isalpha() and w not in self.stopwords]\n",
        "            content_ratio = len(content_words) / max(1, len(words))\n",
        "            # Give a slight weight to earlier sentences\n",
        "            weight = 1.2 if i < 3 else 1.0\n",
        "            results.append((sent, content_ratio * weight))\n",
        "        return results if results else [(\"\", 0.0)]\n",
        "\n",
        "# --- Topic Encoder & Clustering ---\n",
        "class TopicEncoder:\n",
        "    def __init__(self, config: Config, preloaded_model: SentenceTransformer):\n",
        "        self.config = config\n",
        "        self.model = preloaded_model\n",
        "\n",
        "    def encode(self, sentences: List[str]) -> np.ndarray:\n",
        "        clean_sentences = [s.strip() for s in sentences if s.strip()]\n",
        "        if not clean_sentences:\n",
        "            return np.zeros((0, self.model.get_sentence_embedding_dimension()))\n",
        "        return self.model.encode(clean_sentences, batch_size=self.config.batch_size,\n",
        "                                 convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)\n",
        "\n",
        "    def cluster(self, embeddings: np.ndarray) -> Tuple[np.ndarray, int]:\n",
        "        n_samples = len(embeddings)\n",
        "        if n_samples < 2:\n",
        "            return np.zeros(n_samples, dtype=int), 1\n",
        "\n",
        "        # Try HDBSCAN first\n",
        "        if self.config.use_hdbscan:\n",
        "            try:\n",
        "                import hdbscan\n",
        "                clusterer = hdbscan.HDBSCAN(min_cluster_size=self.config.min_cluster_size, metric='euclidean')\n",
        "                labels = clusterer.fit_predict(embeddings)\n",
        "                # Assign noise points (-1) to the nearest valid cluster\n",
        "                if -1 in labels:\n",
        "                    noise_indices = np.where(labels == -1)[0]\n",
        "                    valid_indices = np.where(labels != -1)[0]\n",
        "                    if len(valid_indices) > 0:\n",
        "                        from sklearn.neighbors import NearestNeighbors\n",
        "                        nbrs = NearestNeighbors(n_neighbors=1).fit(embeddings[valid_indices])\n",
        "                        _, indices = nbrs.kneighbors(embeddings[noise_indices])\n",
        "                        labels[noise_indices] = labels[valid_indices[indices.flatten()]]\n",
        "                n_topics = len(set(labels))\n",
        "                if n_topics > 1:\n",
        "                    return labels, n_topics\n",
        "            except ImportError:\n",
        "                print(\"HDBSCAN not found, falling back to KMeans.\")\n",
        "\n",
        "        # Fallback to KMeans\n",
        "        best_labels, best_score, best_k = None, -1, 2\n",
        "        for k in range(2, min(self.config.max_clusters + 1, n_samples)):\n",
        "            kmeans = KMeans(n_clusters=k, random_state=self.config.seed, n_init=10)\n",
        "            current_labels = kmeans.fit_predict(embeddings)\n",
        "            score = silhouette_score(embeddings, current_labels)\n",
        "            if score > best_score:\n",
        "                best_score, best_labels, best_k = score, current_labels, k\n",
        "\n",
        "        return best_labels if best_labels is not None else np.zeros(n_samples, dtype=int), best_k\n",
        "\n",
        "# --- MMR Selector ---\n",
        "class MMRSelector:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "\n",
        "    def select(self, embeddings: np.ndarray, scores: List[float], topics: np.ndarray, n_topics: int) -> List[int]:\n",
        "        n_sentences = len(embeddings)\n",
        "        k = min(self.config.sentences_to_select, n_sentences)\n",
        "\n",
        "        if n_sentences == 0:\n",
        "            return []\n",
        "\n",
        "        selected_indices, candidate_indices = [], list(range(n_sentences))\n",
        "        topic_count = defaultdict(int)\n",
        "\n",
        "        # Normalize importance scores\n",
        "        importance = np.array(scores)\n",
        "        if importance.max() > importance.min():\n",
        "            importance = (importance - importance.min()) / (importance.max() - importance.min())\n",
        "        else:\n",
        "            importance = np.ones_like(importance)\n",
        "\n",
        "        while len(selected_indices) < k and candidate_indices:\n",
        "            best_idx, best_score = -1, -float('inf')\n",
        "\n",
        "            for idx in candidate_indices:\n",
        "                relevance = importance[idx]\n",
        "\n",
        "                # Calculate diversity (max similarity to already selected sentences)\n",
        "                diversity = 1.0\n",
        "                if selected_indices:\n",
        "                    # Cosine similarity\n",
        "                    sim_scores = np.dot(embeddings[idx], embeddings[selected_indices].T)\n",
        "                    diversity = 1.0 - np.max(sim_scores)\n",
        "\n",
        "                # Calculate topic bonus/penalty\n",
        "                topic_id = topics[idx]\n",
        "                if topic_count[topic_id] == 0:\n",
        "                    topic_bonus = self.config.new_topic_bonus\n",
        "                else:\n",
        "                    # Penalize over-representation of a topic\n",
        "                    ideal_ratio = 1.0 / n_topics\n",
        "                    current_ratio = topic_count[topic_id] / max(1, len(selected_indices))\n",
        "                    topic_penalty = self.config.lambda_coverage * (current_ratio - ideal_ratio)\n",
        "                    topic_bonus = -topic_penalty\n",
        "\n",
        "                # Combined score\n",
        "                combined_score = (self.config.lambda_diversity * diversity +\n",
        "                                 0.1 * relevance +\n",
        "                                 topic_bonus)\n",
        "\n",
        "                if combined_score > best_score:\n",
        "                    best_score = combined_score\n",
        "                    best_idx = idx\n",
        "\n",
        "            if best_idx != -1:\n",
        "                selected_indices.append(best_idx)\n",
        "                candidate_indices.remove(best_idx)\n",
        "                topic_count[topics[best_idx]] += 1\n",
        "\n",
        "        return selected_indices\n",
        "\n",
        "# --- Generator ---\n",
        "class Generator:\n",
        "    def __init__(self, config: Config, preloaded_model, preloaded_tokenizer):\n",
        "        self.config = config\n",
        "        self.model = preloaded_model\n",
        "        self.tokenizer = preloaded_tokenizer\n",
        "\n",
        "    def generate(self, text: str) -> str:\n",
        "        if not text or not text.strip():\n",
        "            return \"\"\n",
        "\n",
        "        inputs = self.tokenizer(text, max_length=self.config.max_input_length, truncation=True, return_tensors=\"pt\").to(self.config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            summary_ids = self.model.generate(\n",
        "                inputs['input_ids'],\n",
        "                attention_mask=inputs['attention_mask'],\n",
        "                max_length=self.config.max_output_length,\n",
        "                min_length=self.config.min_output_length,\n",
        "                num_beams=self.config.num_beams,\n",
        "                no_repeat_ngram_size=self.config.no_repeat_ngram,\n",
        "                repetition_penalty=self.config.repetition_penalty,\n",
        "                early_stopping=True\n",
        "            )\n",
        "\n",
        "        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# --- Baseline Summarizer ---\n",
        "class BaselineBERTSummarizer:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        self.model = BertModel.from_pretrained(\"bert-base-uncased\").to(config.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def summarize(self, document: str, top_k: int = 5) -> str:\n",
        "        sentences = sent_tokenize(document)\n",
        "        if not sentences:\n",
        "            return \"\"\n",
        "\n",
        "        inputs = self.tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(self.config.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Get sentence embeddings by averaging token embeddings\n",
        "        sentence_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        document_embedding = sentence_embeddings.mean(dim=0, keepdim=True)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarities = F.cosine_similarity(sentence_embeddings, document_embedding)\n",
        "\n",
        "        # Select top-k sentences\n",
        "        top_indices = similarities.argsort(descending=True)[:top_k]\n",
        "        selected_sentences = [sentences[i] for i in sorted(top_indices)]\n",
        "\n",
        "        return \" \".join(selected_sentences)\n",
        "\n",
        "# --- Evaluator ---\n",
        "class Evaluator:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "        if config.compute_perplexity:\n",
        "            print(\"Loading GPT-2 for perplexity calculation...\")\n",
        "            self.ppl_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "            self.ppl_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(config.device)\n",
        "            self.ppl_model.eval()\n",
        "        else:\n",
        "            self.ppl_model = None\n",
        "\n",
        "        print(\"Loading semantic similarity model for evaluation...\")\n",
        "        self.sem_model = SentenceTransformer(config.encoder_model, device=config.device)\n",
        "\n",
        "    def _calculate_repetition(self, text: str) -> Dict[str, float]:\n",
        "        words = text.lower().split()\n",
        "        if not words:\n",
        "            return {f'{n}gram_repetition': 0.0 for n in [2, 3, 4]}\n",
        "\n",
        "        repetition_metrics = {}\n",
        "        for n in [2, 3, 4]:\n",
        "            if len(words) >= n:\n",
        "                ngrams = [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "                unique_ngrams = len(set(ngrams))\n",
        "                repetition = (1 - unique_ngrams / len(ngrams)) * 100\n",
        "                repetition_metrics[f'{n}gram_repetition'] = repetition\n",
        "            else:\n",
        "                repetition_metrics[f'{n}gram_repetition'] = 0.0\n",
        "\n",
        "        repetition_metrics['unique_word_ratio'] = (len(set(words)) / len(words)) * 100\n",
        "        return repetition_metrics\n",
        "\n",
        "    def evaluate(self, preds: List[str], refs: List[str], sources: List[str]) -> Dict[str, float]:\n",
        "        metrics = {}\n",
        "\n",
        "        # ROUGE Scores\n",
        "        rouge1, rouge2, rougeL = [], [], []\n",
        "        for pred, ref in zip(preds, refs):\n",
        "            score = self.rouge.score(ref, pred)\n",
        "            rouge1.append(score['rouge1'].fmeasure)\n",
        "            rouge2.append(score['rouge2'].fmeasure)\n",
        "            rougeL.append(score['rougeL'].fmeasure)\n",
        "        metrics['rouge1_f1'] = np.mean(rouge1) * 100\n",
        "        metrics['rouge2_f1'] = np.mean(rouge2) * 100\n",
        "        metrics['rougeL_f1'] = np.mean(rougeL) * 100\n",
        "\n",
        "        # BERTScore\n",
        "        if self.config.compute_bertscore:\n",
        "            try:\n",
        "                _, _, F1 = bert_score(preds, refs, lang='en', device=self.config.device, verbose=False)\n",
        "                metrics['bertscore_f1'] = F1.mean().item() * 100\n",
        "            except Exception as e:\n",
        "                print(f\"Could not compute BERTScore: {e}\")\n",
        "                metrics['bertscore_f1'] = 0.0\n",
        "\n",
        "        # Repetition Metrics (calculated per-sample and then averaged)\n",
        "        all_repetitions = [self._calculate_repetition(p) for p in preds]\n",
        "        for key in all_repetitions[0].keys():\n",
        "            metrics[key] = np.mean([r[key] for r in all_repetitions])\n",
        "\n",
        "        # Perplexity\n",
        "        if self.ppl_model:\n",
        "            ppls = []\n",
        "            with torch.no_grad():\n",
        "                for text in preds:\n",
        "                    if not text: continue\n",
        "                    enc = self.ppl_tokenizer(text, return_tensors='pt', truncation=True, max_length=512)\n",
        "                    input_ids = enc.input_ids.to(self.ppl_model.device)\n",
        "                    outputs = self.ppl_model(input_ids, labels=input_ids)\n",
        "                    ppl = torch.exp(outputs.loss).item()\n",
        "                    ppls.append(min(ppl, 1000)) # Cap perplexity to avoid outliers\n",
        "            metrics['perplexity'] = np.mean(ppls) if ppls else 0\n",
        "\n",
        "        # Semantic Consistency\n",
        "        pred_emb = self.sem_model.encode(preds, convert_to_tensor=True, show_progress_bar=False)\n",
        "        src_emb = self.sem_model.encode(sources, convert_to_tensor=True, show_progress_bar=False)\n",
        "        sims = util.cos_sim(pred_emb, src_emb).diagonal()\n",
        "        metrics['semantic_consistency'] = sims.mean().item() * 100\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# --- Visualizer ---\n",
        "class Visualizer:\n",
        "    def __init__(self, out_dir: str):\n",
        "        self.out_dir = Path(out_dir)\n",
        "        self.out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def plot_comparison(self, met_dict: Dict[str, Dict]):\n",
        "        df = pd.DataFrame(met_dict).T\n",
        "        cols = [m for m in ['rouge1_f1','rouge2_f1','rougeL_f1','bertscore_f1','3gram_repetition','semantic_consistency'] if m in df.columns]\n",
        "        if not cols: return\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12,7))\n",
        "        df[cols].plot(kind='bar', ax=ax, width=0.8)\n",
        "        ax.set_title(\"Model Comparison on Key Metrics\", fontsize=16)\n",
        "        ax.set_xlabel(\"Model\")\n",
        "        ax.set_ylabel(\"Score\")\n",
        "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.xticks(rotation=45, ha=\"right\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(self.out_dir / \"metrics_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"✅ Saved: {self.out_dir / 'metrics_comparison.png'}\")"
      ],
      "metadata": {
        "id": "4Qg9yPDviR6x"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 4: MAIN EXECUTION PIPELINE (WITH FINE-TUNING)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Assume these classes are defined in previous cells ---\n",
        "# Preprocessor, TopicEncoder, MMRSelector, Evaluator, BaselineBERTSummarizer, Visualizer\n",
        "\n",
        "# --- UPDATE/REPLACE YOUR GENERATOR CLASS WITH THIS VERSION ---\n",
        "# This version includes a finetune() method with speed optimizations.\n",
        "from transformers import Trainer, TrainingArguments, Seq2SeqTrainingArguments\n",
        "\n",
        "class Generator:\n",
        "    def __init__(self, config: Config, preloaded_model=None, preloaded_tokenizer=None):\n",
        "        self.config = config\n",
        "        # Use a smaller, faster model for fine-tuning if specified\n",
        "        model_checkpoint = config.finetune_model_checkpoint if config.finetune_enabled else config.gen_model\n",
        "\n",
        "        if preloaded_model and model_checkpoint == config.gen_model:\n",
        "            self.model = preloaded_model\n",
        "        else:\n",
        "            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(config.device)\n",
        "\n",
        "        if preloaded_tokenizer and model_checkpoint == config.gen_model:\n",
        "            self.tokenizer = preloaded_tokenizer\n",
        "        else:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "        # For perplexity calculation\n",
        "        if self.config.compute_perplexity:\n",
        "            print(\"Loading GPT-2 for perplexity calculation...\")\n",
        "            self.ppl_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "            self.ppl_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(config.device)\n",
        "            self.ppl_model.eval()\n",
        "            # Set pad token for GPT-2\n",
        "            self.ppl_tokenizer.pad_token = self.ppl_tokenizer.eos_token\n",
        "\n",
        "    def generate(self, text: str) -> str:\n",
        "        # ... (your existing generate method) ...\n",
        "        inputs = self.tokenizer(text, max_length=self.config.max_input_length, truncation=True, return_tensors=\"pt\").to(self.config.device)\n",
        "        summary_ids = self.model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=self.config.max_output_length,\n",
        "            min_length=self.config.min_output_length,\n",
        "            num_beams=self.config.num_beams,\n",
        "            no_repeat_ngram_size=self.config.no_repeat_ngram,\n",
        "            repetition_penalty=self.config.repetition_penalty,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        return self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def finetune(self, train_dataset):\n",
        "        \"\"\"Fine-tunes the model on the provided dataset with speed optimizations.\"\"\"\n",
        "        print(f\"\\n🔥 Starting fine-tuning on {len(train_dataset)} samples...\")\n",
        "        print(f\"   Model: {self.config.finetune_model_checkpoint}\")\n",
        "        log_gpu(\"Before Fine-tuning\")\n",
        "\n",
        "        # --- Tokenization Function ---\n",
        "        def preprocess_function(examples):\n",
        "            # The model expects 'document' and 'summary' columns\n",
        "            model_inputs = self.tokenizer(\n",
        "                examples[\"document\"], max_length=self.config.max_input_length, truncation=True\n",
        "            )\n",
        "            labels = self.tokenizer(\n",
        "                examples[\"summary\"], max_length=self.config.max_output_length, truncation=True\n",
        "            )\n",
        "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "            return model_inputs\n",
        "\n",
        "        # --- Prepare Dataset ---\n",
        "        tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "        tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"document\", \"summary\"])\n",
        "\n",
        "        # --- Training Arguments with Speed Optimizations ---\n",
        "        training_args = Seq2SeqTrainingArguments(\n",
        "            output_dir=self.config.output_dir + \"/finetune\",\n",
        "            num_train_epochs=self.config.finetune_epochs,\n",
        "            per_device_train_batch_size=self.config.finetune_batch_size,\n",
        "\n",
        "            # --- SPEED OPTIMIZATIONS ---\n",
        "            fp16=True,  # Use Mixed Precision (FP16) - HUGE speedup on T4/V100\n",
        "            gradient_accumulation_steps=4, # Simulate larger batch size (e.g., 1 * 4 = 4)\n",
        "            # --- END SPEED OPTIMIZATIONS ---\n",
        "\n",
        "            save_strategy=\"no\", # Don't save checkpoints to save time\n",
        "            logging_steps=10,\n",
        "            report_to=\"none\", # Disable wandb/tensorboard\n",
        "        )\n",
        "\n",
        "        # --- Trainer ---\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_train_dataset,\n",
        "            tokenizer=self.tokenizer,\n",
        "        )\n",
        "\n",
        "        # --- Train ---\n",
        "        trainer.train()\n",
        "\n",
        "        # --- Update the model in the generator ---\n",
        "        self.model = trainer.model\n",
        "        print(\"✅ Fine-tuning complete. Model updated.\")\n",
        "        log_gpu(\"After Fine-tuning\")\n",
        "\n",
        "\n",
        "# --- SummarizationPipeline Class (Updated) ---\n",
        "class SummarizationPipeline:\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        np.random.seed(config.seed)\n",
        "        torch.manual_seed(config.seed)\n",
        "\n",
        "        # Initialize components with preloaded models\n",
        "        self.preproc = Preprocessor(config)\n",
        "        self.encoder = TopicEncoder(config, preloaded_model=encoder_model)\n",
        "        self.selector = MMRSelector(config)\n",
        "        # Generator will load the fine-tuning model if enabled\n",
        "        self.generator = Generator(config, preloaded_model=generator_model, preloaded_tokenizer=generator_tokenizer)\n",
        "        self.evaluator = Evaluator(config)\n",
        "\n",
        "    def finetune(self, dataset):\n",
        "        \"\"\"Public method to trigger fine-tuning.\"\"\"\n",
        "        if self.config.finetune_enabled:\n",
        "            print(\"\\n\" + \"=\"*60)\n",
        "            print(\"FINE-TUNING GENERATOR MODEL\")\n",
        "            print(\"=\"*60)\n",
        "            # Create a small training set\n",
        "            train_dataset = dataset.select(range(min(self.config.finetune_samples, len(dataset))))\n",
        "            self.generator.finetune(train_dataset)\n",
        "        else:\n",
        "            print(\"\\nℹ️ Fine-tuning is disabled in config. Skipping.\")\n",
        "\n",
        "    # ... (rest of the SummarizationPipeline methods: summarize, evaluate_on_dataset, etc. remain the same) ...\n",
        "    def summarize(self, document: str) -> Tuple[str, Dict]:\n",
        "        sent_scores = self.preproc.preprocess(document)\n",
        "        sentences = [s for s, _ in sent_scores]\n",
        "        scores = [sc for _, sc in sent_scores]\n",
        "\n",
        "        if not sentences:\n",
        "            return \"\", {'n_topics': 0, 'coverage_ratio': 0, 'n_selected': 0}\n",
        "\n",
        "        embeddings = self.encoder.encode(sentences)\n",
        "        topic_labels, n_topics = self.encoder.cluster(embeddings)\n",
        "        selected_indices = self.selector.select(embeddings, scores, topic_labels, n_topics)\n",
        "\n",
        "        selected_sentences = [sentences[i] for i in selected_indices]\n",
        "        coverage = len(set(topic_labels[i] for i in selected_indices)) / n_topics if n_topics > 0 else 0\n",
        "\n",
        "        summary_text = \" \".join(selected_sentences)\n",
        "        final_summary = self.generator.generate(summary_text) if summary_text else \"\"\n",
        "\n",
        "        info = {'n_topics': n_topics, 'coverage_ratio': coverage, 'n_selected': len(selected_indices)}\n",
        "        return final_summary, info\n",
        "\n",
        "    def evaluate_on_dataset(self, dataset, name: str) -> Tuple[pd.DataFrame, Dict]:\n",
        "        print(f\"\\n{'='*60}\\nEvaluating: {name}\\n{'='*60}\")\n",
        "        log_gpu(f\"Before {name} evaluation\")\n",
        "\n",
        "        predictions, references, sources = [], [], []\n",
        "        topic_counts, coverage_ratios = [], []\n",
        "\n",
        "        for example in tqdm(dataset, desc=f\"Generating summaries for {name}\"):\n",
        "            doc, ref = example['document'], example['summary']\n",
        "            summary, info = self.summarize(doc)\n",
        "\n",
        "            predictions.append(summary)\n",
        "            references.append(ref)\n",
        "            sources.append(doc)\n",
        "            topic_counts.append(info['n_topics'])\n",
        "            coverage_ratios.append(info['coverage_ratio'])\n",
        "\n",
        "        metrics = self.evaluator.evaluate(predictions, references, sources)\n",
        "\n",
        "        results_df = pd.DataFrame({\n",
        "            'source': sources,\n",
        "            'reference': references,\n",
        "            'prediction': predictions,\n",
        "            'n_topics': topic_counts,\n",
        "            'coverage_ratio': coverage_ratios\n",
        "        })\n",
        "\n",
        "        log_gpu(f\"After {name} evaluation\")\n",
        "        self._print_metrics(metrics, name)\n",
        "        return results_df, metrics\n",
        "\n",
        "    def evaluate_baseline(self, dataset) -> Tuple[pd.DataFrame, Dict]:\n",
        "        print(f\"\\n{'='*60}\\nEvaluating: BERT Baseline\\n{'='*60}\")\n",
        "        log_gpu(\"Before baseline evaluation\")\n",
        "\n",
        "        baseline = BaselineBERTSummarizer(self.config)\n",
        "        predictions, references, sources = [], [], []\n",
        "\n",
        "        for example in tqdm(dataset, desc=\"Generating baseline summaries\"):\n",
        "            doc, ref = example['document'], example['summary']\n",
        "            summary = baseline.summarize(doc)\n",
        "            predictions.append(summary)\n",
        "            references.append(ref)\n",
        "            sources.append(doc)\n",
        "\n",
        "        metrics = self.evaluator.evaluate(predictions, references, sources)\n",
        "\n",
        "        results_df = pd.DataFrame({\n",
        "            'source': sources,\n",
        "            'reference': references,\n",
        "            'prediction': predictions,\n",
        "            'n_topics': [1] * len(predictions), # Placeholder\n",
        "            'coverage_ratio': [1.0] * len(predictions) # Placeholder\n",
        "        })\n",
        "\n",
        "        log_gpu(\"After baseline evaluation\")\n",
        "        self._print_metrics(metrics, \"BERT Baseline\")\n",
        "        return results_df, metrics\n",
        "\n",
        "    def _print_metrics(self, metrics: Dict, title: str):\n",
        "        print(f\"\\n{title.upper()} METRICS\")\n",
        "        print(\"-\" * 40)\n",
        "        groups = {\n",
        "            'ROUGE': ['rouge1_f1', 'rouge2_f1', 'rougeL_f1'],\n",
        "            'Semantic Quality': ['bertscore_f1', 'semantic_consistency'],\n",
        "            'Redundancy': ['2gram_repetition', '3gram_repetition', '4gram_repetition', 'unique_word_ratio'],\n",
        "            'Fluency': ['perplexity'],\n",
        "        }\n",
        "        for group_name, keys in groups.items():\n",
        "            print(f\"\\n{group_name}:\")\n",
        "            for key in keys:\n",
        "                if key in metrics:\n",
        "                    print(f\"  {key:<25}: {metrics[key]:>8.2f}\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "\n",
        "# --- Main Execution (Updated) ---\n",
        "def main():\n",
        "    print(\"=\"*80)\n",
        "    print(\"HIERARCHICAL TOPIC-AWARE SUMMARIZATION - REMASTERED\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    optimize_gpu()\n",
        "    log_gpu(\"Initial State\")\n",
        "\n",
        "    config = Config()\n",
        "    out_dir = Path(config.output_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load Dataset\n",
        "    print(f\"\\nLoading {config.dataset_name} dataset...\")\n",
        "    dataset = load_dataset(config.dataset_name, config.dataset_version, split=config.split, trust_remote_code=True)\n",
        "    dataset = dataset.shuffle(seed=config.seed).select(range(min(config.n_samples, len(dataset))))\n",
        "    # Rename columns for consistency\n",
        "    dataset = dataset.map(lambda x: {\"document\": x[\"article\"], \"summary\": x[\"highlights\"]}, remove_columns=[\"article\", \"highlights\"])\n",
        "    print(f\"✅ Loaded {len(dataset)} samples.\")\n",
        "\n",
        "    # Initialize Pipeline\n",
        "    pipeline = SummarizationPipeline(config)\n",
        "\n",
        "    # --- NEW: FINE-TUNING STEP ---\n",
        "    pipeline.finetune(dataset)\n",
        "\n",
        "    # --- Run Evaluations ---\n",
        "    # 1. Hierarchical Model (now potentially fine-tuned)\n",
        "    hier_df, hier_metrics = pipeline.evaluate_on_dataset(dataset, \"Hierarchical Model (Fine-tuned)\" if config.finetune_enabled else \"Hierarchical Model\")\n",
        "    hier_df.to_csv(out_dir / \"hierarchical_results.csv\", index=False)\n",
        "    print(f\"✅ Saved results to {out_dir / 'hierarchical_results.csv'}\")\n",
        "\n",
        "    # 2. BERT Baseline\n",
        "    base_df, base_metrics = pipeline.evaluate_baseline(dataset)\n",
        "    base_df.to_csv(out_dir / \"baseline_results.csv\", index=False)\n",
        "    print(f\"✅ Saved results to {out_dir / 'baseline_results.csv'}\")\n",
        "\n",
        "    # --- Save and Visualize Results ---\n",
        "    all_metrics_df = pd.DataFrame({\"Hierarchical\": hier_metrics, \"BERT_Baseline\": base_metrics}).T\n",
        "    all_metrics_df.to_csv(out_dir / \"all_metrics_summary.csv\")\n",
        "    print(f\"✅ Saved metric summary to {out_dir / 'all_metrics_summary.csv'}\")\n",
        "\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    visualizer = Visualizer(str(out_dir / \"plots\"))\n",
        "    visualizer.plot_comparison({\"Hierarchical\": hier_metrics, \"BERT_Baseline\": base_metrics})\n",
        "\n",
        "    print(\"\\n🎉 EVALUATION COMPLETE!\")\n",
        "    print(\"Results and plots are saved in the './results' directory.\")\n",
        "    log_gpu(\"Final State\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "36cea54bcfcb4b28968e676741be4d9d",
            "dce11f2421854ce49d02a27881373720",
            "849e3ebf80d24fd9902514dd56667232",
            "965b4f79f0d64a1883d4fd96a335959d",
            "27d7fa52c5cf48919f5f3fda8f3bacd1",
            "2fd29668f4b34237a2a75cf084413093",
            "16fcf97ed07147f38a2de323470c2783",
            "d23f31cbc77f4f229df1364c51c9bea7",
            "5beb5d369bd44e63be8c48f29ccfd294",
            "adbaf0b1ff6f477095eeab5465f21cfb",
            "f523e3b0de2d4a118b7b47c21d44d013",
            "ae498a45015a42edacf3299d6e6ad25c",
            "56e185825c7444b1be82383f6bda5740",
            "b1a59a75011e4532942b84cbc2502ccd",
            "cf929ba6f86845dbaa85ec5e596bc846",
            "af2c7bcb64084efdbe6c983d7bff64dd",
            "22bb25a3219d477abddecf3bbc9ef5e7",
            "5fedf0d354b04a97a2987db605bd8357",
            "32a9a19720ce4205893c2e829d72b40a",
            "461fb9c8840f42428a386b80bd58a586",
            "6db84b91369e44b79c5525a8dff3f1ac",
            "a6bf051eb105480487f8144b4c946c75",
            "a98935bdbd644f4d8ab6607b57debdec",
            "675e57c90f8a4fa3ad97f4fb1004e881",
            "a85e35bb6a3046e1bdad4f976e175e51",
            "7ad0faa1066944a68d3b2fcfe3bd8c9c",
            "e44a2315d05e4d16b5379c0fd77400cf",
            "17ee29857a504741a0719d3627a1faf0",
            "2529c97650744368b41bd3363833a829",
            "eabb0be71d5c454ebe596c18cee1793a",
            "17700e9da54e4948bf110f740a2c3ff6",
            "ef56813a22de40449287d3957533637b",
            "8af1f9c822ab47f6bac8cae53a1db485"
          ]
        },
        "id": "58daV2kgiUzU",
        "outputId": "fcdce215-4696-44b7-8c35-05a709ec8065"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "HIERARCHICAL TOPIC-AWARE SUMMARIZATION - REMASTERED\n",
            "================================================================================\n",
            "✅ GPU optimization enabled.\n",
            "[GPU] Initial State: Allocated 9.07 GB, Reserved 9.77 GB\n",
            "✅ Config object initialized. Using device: cuda\n",
            "🔥 Fine-tuning ENABLED. Will use model: sshleifer/distilbart-cnn-12-6\n",
            "\n",
            "Loading cnn_dailymail dataset...\n",
            "✅ Loaded 500 samples.\n",
            "Loading GPT-2 for perplexity calculation...\n",
            "Loading GPT-2 for perplexity calculation...\n",
            "Loading semantic similarity model for evaluation...\n",
            "\n",
            "============================================================\n",
            "FINE-TUNING GENERATOR MODEL\n",
            "============================================================\n",
            "\n",
            "🔥 Starting fine-tuning on 500 samples...\n",
            "   Model: sshleifer/distilbart-cnn-12-6\n",
            "[GPU] Before Fine-tuning: Allocated 11.58 GB, Reserved 11.91 GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36cea54bcfcb4b28968e676741be4d9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 02:22, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.348700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.102800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.821700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.887600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.798300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.842500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.844000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.986200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.718000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.669300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.924400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.847600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.120900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.016100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.093300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.091300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.069200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.031300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.954600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.991000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.105100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.118900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.157100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.020900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fine-tuning complete. Model updated.\n",
            "[GPU] After Fine-tuning: Allocated 9.64 GB, Reserved 12.49 GB\n",
            "\n",
            "============================================================\n",
            "Evaluating: Hierarchical Model (Fine-tuned)\n",
            "============================================================\n",
            "[GPU] Before Hierarchical Model (Fine-tuned) evaluation: Allocated 9.64 GB, Reserved 12.49 GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating summaries for Hierarchical Model (Fine-tuned):   0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae498a45015a42edacf3299d6e6ad25c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPU] After Hierarchical Model (Fine-tuned) evaluation: Allocated 7.37 GB, Reserved 13.51 GB\n",
            "\n",
            "HIERARCHICAL MODEL (FINE-TUNED) METRICS\n",
            "----------------------------------------\n",
            "\n",
            "ROUGE:\n",
            "  rouge1_f1                :    43.13\n",
            "  rouge2_f1                :    22.04\n",
            "  rougeL_f1                :    30.35\n",
            "\n",
            "Semantic Quality:\n",
            "  bertscore_f1             :    88.02\n",
            "  semantic_consistency     :    79.17\n",
            "\n",
            "Redundancy:\n",
            "  2gram_repetition         :     0.96\n",
            "  3gram_repetition         :     0.02\n",
            "  4gram_repetition         :     0.01\n",
            "  unique_word_ratio        :    83.26\n",
            "\n",
            "Fluency:\n",
            "  perplexity               :    38.89\n",
            "----------------------------------------\n",
            "✅ Saved results to results/hierarchical_results.csv\n",
            "\n",
            "============================================================\n",
            "Evaluating: BERT Baseline\n",
            "============================================================\n",
            "[GPU] Before baseline evaluation: Allocated 7.37 GB, Reserved 13.51 GB\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating baseline summaries:   0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a98935bdbd644f4d8ab6607b57debdec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GPU] After baseline evaluation: Allocated 7.78 GB, Reserved 14.44 GB\n",
            "\n",
            "BERT BASELINE METRICS\n",
            "----------------------------------------\n",
            "\n",
            "ROUGE:\n",
            "  rouge1_f1                :    31.04\n",
            "  rouge2_f1                :    10.61\n",
            "  rougeL_f1                :    18.92\n",
            "\n",
            "Semantic Quality:\n",
            "  bertscore_f1             :    85.60\n",
            "  semantic_consistency     :    82.36\n",
            "\n",
            "Redundancy:\n",
            "  2gram_repetition         :     7.18\n",
            "  3gram_repetition         :     4.04\n",
            "  4gram_repetition         :     3.11\n",
            "  unique_word_ratio        :    70.15\n",
            "\n",
            "Fluency:\n",
            "  perplexity               :    32.80\n",
            "----------------------------------------\n",
            "✅ Saved results to results/baseline_results.csv\n",
            "✅ Saved metric summary to results/all_metrics_summary.csv\n",
            "\n",
            "Generating visualizations...\n",
            "✅ Saved: results/plots/metrics_comparison.png\n",
            "\n",
            "🎉 EVALUATION COMPLETE!\n",
            "Results and plots are saved in the './results' directory.\n",
            "[GPU] Final State: Allocated 7.37 GB, Reserved 14.44 GB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7VIsPI0B/ZOxO5OJPaH5Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "36cea54bcfcb4b28968e676741be4d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dce11f2421854ce49d02a27881373720",
              "IPY_MODEL_849e3ebf80d24fd9902514dd56667232",
              "IPY_MODEL_965b4f79f0d64a1883d4fd96a335959d"
            ],
            "layout": "IPY_MODEL_27d7fa52c5cf48919f5f3fda8f3bacd1",
            "tabbable": null,
            "tooltip": null
          }
        },
        "dce11f2421854ce49d02a27881373720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_2fd29668f4b34237a2a75cf084413093",
            "placeholder": "​",
            "style": "IPY_MODEL_16fcf97ed07147f38a2de323470c2783",
            "tabbable": null,
            "tooltip": null,
            "value": "Map: 100%"
          }
        },
        "849e3ebf80d24fd9902514dd56667232": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_d23f31cbc77f4f229df1364c51c9bea7",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5beb5d369bd44e63be8c48f29ccfd294",
            "tabbable": null,
            "tooltip": null,
            "value": 500
          }
        },
        "965b4f79f0d64a1883d4fd96a335959d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_adbaf0b1ff6f477095eeab5465f21cfb",
            "placeholder": "​",
            "style": "IPY_MODEL_f523e3b0de2d4a118b7b47c21d44d013",
            "tabbable": null,
            "tooltip": null,
            "value": " 500/500 [00:01&lt;00:00, 258.80 examples/s]"
          }
        },
        "27d7fa52c5cf48919f5f3fda8f3bacd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd29668f4b34237a2a75cf084413093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16fcf97ed07147f38a2de323470c2783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "d23f31cbc77f4f229df1364c51c9bea7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5beb5d369bd44e63be8c48f29ccfd294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adbaf0b1ff6f477095eeab5465f21cfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f523e3b0de2d4a118b7b47c21d44d013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "ae498a45015a42edacf3299d6e6ad25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56e185825c7444b1be82383f6bda5740",
              "IPY_MODEL_b1a59a75011e4532942b84cbc2502ccd",
              "IPY_MODEL_cf929ba6f86845dbaa85ec5e596bc846"
            ],
            "layout": "IPY_MODEL_af2c7bcb64084efdbe6c983d7bff64dd",
            "tabbable": null,
            "tooltip": null
          }
        },
        "56e185825c7444b1be82383f6bda5740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_22bb25a3219d477abddecf3bbc9ef5e7",
            "placeholder": "​",
            "style": "IPY_MODEL_5fedf0d354b04a97a2987db605bd8357",
            "tabbable": null,
            "tooltip": null,
            "value": "Generating summaries for Hierarchical Model (Fine-tuned): 100%"
          }
        },
        "b1a59a75011e4532942b84cbc2502ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_32a9a19720ce4205893c2e829d72b40a",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_461fb9c8840f42428a386b80bd58a586",
            "tabbable": null,
            "tooltip": null,
            "value": 500
          }
        },
        "cf929ba6f86845dbaa85ec5e596bc846": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_6db84b91369e44b79c5525a8dff3f1ac",
            "placeholder": "​",
            "style": "IPY_MODEL_a6bf051eb105480487f8144b4c946c75",
            "tabbable": null,
            "tooltip": null,
            "value": " 500/500 [13:39&lt;00:00,  1.57s/it]"
          }
        },
        "af2c7bcb64084efdbe6c983d7bff64dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22bb25a3219d477abddecf3bbc9ef5e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fedf0d354b04a97a2987db605bd8357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "32a9a19720ce4205893c2e829d72b40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "461fb9c8840f42428a386b80bd58a586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6db84b91369e44b79c5525a8dff3f1ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6bf051eb105480487f8144b4c946c75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "a98935bdbd644f4d8ab6607b57debdec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_675e57c90f8a4fa3ad97f4fb1004e881",
              "IPY_MODEL_a85e35bb6a3046e1bdad4f976e175e51",
              "IPY_MODEL_7ad0faa1066944a68d3b2fcfe3bd8c9c"
            ],
            "layout": "IPY_MODEL_e44a2315d05e4d16b5379c0fd77400cf",
            "tabbable": null,
            "tooltip": null
          }
        },
        "675e57c90f8a4fa3ad97f4fb1004e881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_17ee29857a504741a0719d3627a1faf0",
            "placeholder": "​",
            "style": "IPY_MODEL_2529c97650744368b41bd3363833a829",
            "tabbable": null,
            "tooltip": null,
            "value": "Generating baseline summaries: 100%"
          }
        },
        "a85e35bb6a3046e1bdad4f976e175e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_eabb0be71d5c454ebe596c18cee1793a",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17700e9da54e4948bf110f740a2c3ff6",
            "tabbable": null,
            "tooltip": null,
            "value": 500
          }
        },
        "7ad0faa1066944a68d3b2fcfe3bd8c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_ef56813a22de40449287d3957533637b",
            "placeholder": "​",
            "style": "IPY_MODEL_8af1f9c822ab47f6bac8cae53a1db485",
            "tabbable": null,
            "tooltip": null,
            "value": " 500/500 [01:03&lt;00:00,  6.76it/s]"
          }
        },
        "e44a2315d05e4d16b5379c0fd77400cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17ee29857a504741a0719d3627a1faf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2529c97650744368b41bd3363833a829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "eabb0be71d5c454ebe596c18cee1793a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17700e9da54e4948bf110f740a2c3ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef56813a22de40449287d3957533637b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8af1f9c822ab47f6bac8cae53a1db485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}